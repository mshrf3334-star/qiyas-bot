# ask_qiyas_ai.py
# â€” ÙŠØ¹Ù…Ù„ Ø¨Ø¯ÙˆÙ† tenacity â€” ÙŠØ³ØªØ®Ø¯Ù… Ø¥Ø¹Ø§Ø¯Ø© Ù…Ø­Ø§ÙˆÙ„Ø© Ø¨Ø³ÙŠØ·Ø©

import os
import logging
import asyncio
from typing import Optional

from telegram import Update
from telegram.ext import ContextTypes
from openai import OpenAI

# ===== Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù…Ù† Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦ÙŠØ© =====
AI_API_KEY = os.getenv("AI_API_KEY")
AI_MODEL   = os.getenv("AI_MODEL", "gpt-4o-mini")

logger = logging.getLogger(__name__)
_client: Optional[OpenAI] = None


def _get_client() -> Optional[OpenAI]:
    """ØªÙ‡ÙŠØ¦Ø© Ø¹Ù…ÙŠÙ„ OpenAI Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø©."""
    global _client
    if not AI_API_KEY:
        return None
    if _client is None:
        _client = OpenAI(api_key=AI_API_KEY)
    return _client


async def ask_qiyas_ai_handler(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """
    Ø£Ù…Ø±: /ask_ai Ø³Ø¤Ø§Ù„Ùƒ Ù‡Ù†Ø§
    Ø£Ùˆ: Ø±Ø¯Ù‘ Ø¨Ø§Ù„Ø£Ù…Ø± /ask_ai Ø¹Ù„Ù‰ Ø±Ø³Ø§Ù„Ø© ØªØ­ÙˆÙŠ Ø§Ù„Ø³Ø¤Ø§Ù„.
    """
    # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù†Øµ Ø§Ù„Ø³Ø¤Ø§Ù„
    question = " ".join(context.args) if context.args else None
    if not question and update.message and update.message.reply_to_message:
        question = update.message.reply_to_message.text

    if not question:
        await update.effective_message.reply_html(
            "Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ø¨Ø¹Ø¯ Ø§Ù„Ø£Ù…Ø±:<code> /ask_ai Ø³Ø¤Ø§Ù„Ùƒ </code>\n"
            "Ø£Ùˆ Ø±Ø¯Ù‘ Ø¨Ø§Ù„Ø£Ù…Ø± Ø¹Ù„Ù‰ Ø±Ø³Ø§Ù„Ø© ØªØ­ØªÙˆÙŠ Ø³Ø¤Ø§Ù„Ùƒ."
        )
        return

    if not AI_API_KEY:
        await update.effective_message.reply_text(
            "ğŸ›‘ Ù„Ù… ÙŠØªÙ… Ø¶Ø¨Ø· Ù…ÙØªØ§Ø­ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ (AI_API_KEY)."
        )
        return

    try:
        answer = await _ask_llm(question)
        if not answer:
            answer = "Ù„Ù… Ø£Ø³ØªØ·Ø¹ ØªÙˆÙ„ÙŠØ¯ Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ø¢Ù†. Ø­Ø§ÙˆÙ„ Ù„Ø§Ø­Ù‚Ù‹Ø§."
        # Ø­Ø¯ Ø§Ù„ØªÙ„ØºØ±Ø§Ù… 4096 Ø­Ø±Ù Ù„Ù„Ø±Ø³Ø§Ù„Ø© Ø§Ù„ÙˆØ§Ø­Ø¯Ø©
        await update.effective_message.reply_text(answer[:4096])
    except Exception as e:
        logger.exception("ask_ai error: %s", e)
        await update.effective_message.reply_text("Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©. Ø­Ø§ÙˆÙ„ Ù„Ø§Ø­Ù‚Ù‹Ø§.")


async def _ask_llm(prompt: str) -> str:
    """
    Ù†Ø¯Ø§Ø¡ LLM Ù…Ø¹ Ù…Ø­Ø§ÙˆÙ„Ø§Øª Ø¨Ø³ÙŠØ·Ø© (3 Ù…Ø­Ø§ÙˆÙ„Ø§Øª Ø¨ØªØ£Ø®ÙŠØ±Ø§Øª 0s/1.5s/3s).
    """
    client = _get_client()
    if not client:
        return "Ø§Ù„Ù…ÙØªØ§Ø­ ØºÙŠØ± Ù…Ù‡ÙŠØ£."

    delays = [0.0, 1.5, 3.0]
    last_err: Optional[Exception] = None

    for delay in delays:
        if delay:
            await asyncio.sleep(delay)
        try:
            resp = client.chat.completions.create(
                model=AI_MODEL,
                temperature=0.3,
                messages=[
                    {
                        "role": "system",
                        "content": (
                            "Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ù‚ÙŠØ§Ø³ Ø°ÙƒÙŠ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©: Ù…ÙˆØ¬Ø²ØŒ Ø¯Ù‚ÙŠÙ‚ØŒ ÙˆÙŠØ´Ø±Ø­ Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø§Ø¬Ø©."
                        ),
                    },
                    {"role": "user", "content": prompt.strip()},
                ],
            )
            content = (resp.choices[0].message.content or "").strip()
            return content
        except Exception as e:
            last_err = e
            logger.warning("LLM call failed (will retry): %s", e)

    # Ø¥Ø°Ø§ ÙØ´Ù„Øª ÙƒÙ„ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø§Øª
    raise last_err if last_err else RuntimeError("LLM call failed")
